[{"categories":null,"contents":" The Data Analytics Consulting Virtual Internship by KPMG was my initial guide into the world of data analytics.\nThe entire experience was divided into three modules - Data Quality Assessment, Data Insights, and Presentation.\nData Quality Assessment dealed with the preparation and cleaning of data to get it ready for analysis. A dataset related to bicycle sales was provided.\nData Insights involved figuring out the high value customers based on customer demographics and attributes.\nData Insights and Presentation was all about putting up the data in a visual format, using Power BI\nThe entire course taught me how to analyse data using MS Excel. I also got to use a visualisation software, Power BI, which allowed me to present the data in an attractive manner.\n","permalink":"https://Advaith2049.github.io/portfolio_website/projects/contributions/kpmg/","tags":["Data Science","Data Analytics","KPMG","Internship","Forage"],"title":"Data Analytics Consulting Virtual Internship"},{"categories":null,"contents":"This talk looked at Liberty Mutual’s transformation to Continuous Integration, Continuous Delivery, and DevOps. For a large, heavily regulated industry, this task can not only be daunting, but viewed by many as impossible. Often, organizations try to reduce the friction through micro-fixes, but Eddie’s team asked how to change the culture to reduce the friction and concluded with the following final points:\nDon’t mandate DevOps. Give employees the chance to master their discipline with examples to set and follow. Favor deep end-to-end accomplishments over broad but incremental steps forward. Focus on taking the right teams far before encouraging broad adoption. Centralize the platforms and tools that your teams shouldn’t be thinking about. Provide foundational services/commodities and let teams stay on purpose. Incorporate contributions from everyone; don’t stifle autonomy. Stay open to new ways of working. Challenge security policies, but respect intentions. Find new ways to enforce concerns without abandoning precaution. ","permalink":"https://Advaith2049.github.io/portfolio_website/publications/alldaydevops/","tags":["DevOps","Continuous Integration","Continuous Delivery","CI/CD pipelines","agile","Culture"],"title":"Organically DevOps: Building Quality and Security into the Software Supply Chain at Liberty Mutual"},{"categories":null,"contents":" The AWS Solutions Architect Virtual Experience Program was an insightful and detailed look into understanding the work of a Solutions Architect.\nThe Program gave a wonderful opportunity to put myself in the shoes of an Architect and perform tasks that simulated the real life work.\nThe main aim of the module was to design a simple, scalable, hosting architecture and compile it as a letter to the client.\nThe task gave me an understanding of the various services that AWS provides like CodePipeline, RDS, and Elastic Load Balancing among others.\nI also gained a fair share of knowledge about executing appliations, by various components like EC2, Fargate, and Lambda among others.\n","permalink":"https://Advaith2049.github.io/portfolio_website/projects/contributions/aws/","tags":["AWS","Solutions Architect"],"title":"Solutions Architect Virtual Experience Program"},{"categories":null,"contents":" The Data Science \u0026amp; Machine Learning Internship provided by UNP (United Network of Professionals) was an excellent introduction to the foundational topics of data science.\nIt initially tackled the basics of Python language, and informed about various aspects like lists, loops, and several packages including pandas, numpy, and seaborn among others.\nThe course futher progressed to introducing datasets into picture, on which we were supposed to apply basic algorithms like linear regression and logistic regression.\nFurther, I was able to learn various Machine Learning algorithms like Decision Trees, Random Forest, Support Vector Machines, K-Nearest Neighbours, etc.\nWe were asked to do a project as a team, and further explain it using a powerpoint presentation in an offline platform.\nI was able to complete two major machine learning projects - The Used Car Price Prediction and The Bank Term Deposit Prediction.\n","permalink":"https://Advaith2049.github.io/portfolio_website/projects/contributions/unp/","tags":["Data Science","Analytics","Python","Machine Learning"],"title":"Data Science And Machine Learning Internship"},{"categories":null,"contents":"Used Car Price Prediction: An Overview Cleaned the dataset, removed outliers, and pre-processed the data in a format suitable to the objective\nPerformed extensive EDA or Exploratory Data Analysis to understand the relationships between the variables and data points\nFit the processed data into various models including Linear Regression, Random Forest, KNN, SVM, among several other algorithms, to make predictions and arrive at the best model through comparison\nCharted a comparison table for the ease of determining the best model and accuracy\nCodes and Resources Used Programming Language: Python 3.11.1\nPackages: pandas, numpy, seaborn, matplotlib, scipy, sklearn, statsmodels, tensorflow\nAbout the Dataset The Car Price Prediction dataset has a total of 18 columns and 19237 rows.\nThere are no null values across the data, and the data types range from \u0026lsquo;object\u0026rsquo; type to \u0026lsquo;integer\u0026rsquo; type.\nThe target (or) dependent variable which needs to be predicted is the column \u0026lsquo;Price\u0026rsquo;\nData Cleaning After the data was loaded in with pandas, it was cleaned, and the following changes were made:\nColumns like ‘Prod. year’, ‘Engine volume’, ‘Gear box type’, among others were renamed to more simpler and suitable names\nData columns were divided into numeric type and object type for the ease of working and conversion\n‘ID’ column was dropped\n‘km’ string was dropped from the ‘Mileage’ column and was converted into a float column\n‘Engine_vol’ column was standardized by the creation of another column, ‘Engine_type’, which stored binary values of ‘Turbo’ or ‘Non-Turbo’\nOutliers were detected in the columns – Levy, Cylinders, Airbags, Engine_vol, Mileage, and Price, and subsequently removed\nEDA (Exploratory Data Analysis) Some highlights of the EDA are as follows:\nFeature Selection and Pre-Processing VIF or Variance Inflation Factor was utilised to select the features that would influence the target variable the most.\n‘Cylinders’, ‘Engine_vol’, and ‘Levy’ were dropped to ensure a better VIF score and less multicolinearity.\nModel Building The categorical variables were first transformed into dummy variables using ‘OneHotEncoder’ and ‘ColumnTransformer’. Then, the entire dataset was split into training and testing datasets of the ratio 80:20. Multiple models were applied and MAE or Mean Absolute Error was the metric used to evaluate these models.\nThe applied models were:\nLinear Regression Boosting (Gradient, Ada Boost, XG Boost) Bagging Decision Tree Regressor Random Forest Regressor KNN SVM Neural Networks Model Performance The Bagging Repressor outperformed all the models and the model comparison table is as follows:\n","permalink":"https://Advaith2049.github.io/portfolio_website/projects/creations/car_price/","tags":["Python","Machine Learning","Linear Regression"],"title":"Used Car Price Prediction using Machine Learning"},{"categories":null,"contents":"Intro Doesn\u0026rsquo;t matter whether it\u0026rsquo;s a CakePHP app for a client, your own personal CMS, or any other web based application. If your passing around passwords or other sensitive info you should really implement SSL. SSL provides 2 main perks to your visitors.\nFirst it encrypts all communication that flies across the web. This prevents curious or devious billies from getting your secrets. Secondly it ensures to the user that your server is in fact who it claims, and not a nasty \u0026lsquo;man in the middle\u0026quot; attack. Finally it gives your site that touch of class\u0026hellip;. which of course a classy person like yourself relies on. Once you implement SSL certificates on your server you\u0026rsquo;ll want to require secure connections using Apache\u0026rsquo;s rewrite module. Now I won\u0026rsquo;t dwell on the creation and signing of certificates, its already well documented. If your just starting out though,heres a few links I recommend;\nCreating self-signed certificates (free, but should only be used internally or for testing, users will; see an \u0026lsquo;Untrusted\u0026quot; warning) Requesting a CA Signed certificate (not free, but the final certificate is trusted and seamless for users) The second link uses the schools internal CA, you will need to pay a public CA like Entrust or Verisign. All of this information is aimed at \u0026rsquo;nix or solaris servers running apache. Why? cause a production windows server is laughable :-p\nNow that you have a certificate, whats next? So there you are you have a shiny new Certificate and Server key, how do you force visitors to your apache driven site to use the SSL? You copied the certificates into the appropite locations right? And you have made the needed changes in httpd.conf right? So now when you view https://example.com you see a \u0026rsquo;trusted\u0026rsquo; warning or your site right? If No to any of these than this article does a pretty good job of outlining those steps.\nThe SSL Works, How do I force connections to use it? First you need to decide if you want to force every page on your site to use SSL, or only a particular sub-domain, or maybe just your admin directory. Since the overhead is minimal there is no harm is forcing the entire domain to leverage SSL, but if it is a self-signed certificate for your personal use than you\u0026rsquo;ll most certainly want to restrict its use to your own areas. This prevents users from seeing that nasty warning \u0026ldquo;This server is not trusted\u0026rdquo; You\u0026rsquo;ll know if your using SSL because the url prefix changes from http to https (s for secure).\nForcing entire domain to use SSL You want any visit, any where to use ssl. This probably the simplest solution. Create or append to your htaccess file in the top directory of your server. Some people use a port check (80 is typically http, while 443 is https) but if you have alernate configs or the user just adds :8080 to the end of the url this method is useless. Instead check whether the https environmental variable is set, if not then redirect.\nRewriteCond %{HTTPS} !=on RewriteRule ^(.*)$ https://%{SERVER_NAME}$1 \\[R,L\\] Forcing sub-domains to use SSL Maybe you only want mysecretarea.example.com to use SSL, that\u0026rsquo;s easy enough. Its the same premise as above, but you move the htaccess file into the directory that corresponds to the subdomain. Also change the second line like below;\nRewriteCond %{HTTPS} !=on RewriteRule ^(.*)$ https://mysecretarea.%{SERVER_NAME}$1 \\[R,L\\] Forcing a directory to use SSL This method cn get a little hairier if your using aliases or redirects on top of this one. You\u0026rsquo;ll need to consider what order the commands are read. The basic principle is like so. You want all visits to example.com/admin to use ssl. Create a htaccess file in the parent directory. Again will check for the https variable, but this time we also check for the sub-directory to be in the path.\nRewriteCond %{HTTPS} !=on RewriteRule ^/admin/(.*)$ https://%{SERVER_NAME}/admin/$1 \\[R,L\\] ","permalink":"https://Advaith2049.github.io/portfolio_website/blog/force-ssl/","tags":["apache","apache","redirect","rewrite","ssl","web development"],"title":"Forcing Visits to use SSL"},{"categories":null,"contents":"Bank Term Deposit Prediction: An Overview Cleaned the dataset, removed outliers, and pre-processed the data in a format suitable to the objective\nPerformed extensive EDA or Exploratory Data Analysis to understand the relationships between the variables and data points\nFit the processed data into various models including Logistic Regression, Random Forest, KNN, SVM, among several other algorithms, to make predictions and arrive at the best model through comparison\nCharted a comparison table for the ease of determining the best model and accuracy\nCodes and Resources Used Programming Language: Python 3.11.1\nPackages: pandas, numpy, seaborn, matplotlib, scipy, sklearn, statsmodels, tensorflow\nAbout the Dataset The data is related to direct marketing campaigns of a Portugese banking institution.\nThe marketing campaigns were based on phone calls.\nOften, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be subscribed (\u0026lsquo;yes\u0026rsquo;) or not (\u0026rsquo;no\u0026rsquo;) subscribed.\nTarget or dependent variable is \u0026lsquo;y\u0026rsquo; column, a binary feature which represents if client has subscribed to a term deposit (\u0026lsquo;yes\u0026rsquo;/\u0026rsquo;no\u0026rsquo;)\nData Cleaning After the data was loaded in with pandas, it was cleaned, and the following changes were made:\nColumn ‘Duration’ was converted into hours format for ease of analysis\nOutliers from the column ‘Duration’ were removed using IQR or Inter Quartile Range method\nEDA (Exploratory Data Analysis) Some highlights of the EDA are as follows:\nModel Building The categorical variables were first transformed into dummy variables using ‘get_dummies’. Then, the entire dataset was split into training and testing datasets of the ratio 80:20. Multiple models were applied and MAE or Mean Absolute Error was the metric used to evaluate these models.\nRFE or Recursive Feature Elimination was used to eliminate the unimportant variables. RFE recursively iterates through the data and gives the best possible combination of columns to undergo logistic regression\nThe applied models were:\nLogistic Regression Boosting (Gradient, Ada Boost, XG Boost) Bagging Decision Tree Regressor Random Forest Regressor KNN SVM Neural Networks Model Performance The XG Boosting Classifier outperformed all the models.\n","permalink":"https://Advaith2049.github.io/portfolio_website/projects/creations/term_deposit/","tags":["Python","Machine Learning","Logistic Regression"],"title":"Bank Term Deposit Prediction using Machine Learning"},{"categories":null,"contents":"IMDB Web Scraping \u0026amp; Analysis\n","permalink":"https://Advaith2049.github.io/portfolio_website/projects/creations/imdb/","tags":["Python","IMDB","Web Scraping","EDA","Exploratory Data Analysis"],"title":"IMDB Web Scraping \u0026 Analysis"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"https://Advaith2049.github.io/portfolio_website/search/","tags":null,"title":"Search Results"}]